# Transformer from Scratch in PyTorch

![transf](https://github.com/user-attachments/assets/151490f2-9ef0-4771-afcb-6e0f1f4ff259)


This project implements the Transformer model architecture from the ground up using PyTorch, based on the paper "Attention Is All You Need" by Vaswani et al. The implementation focuses on creating a machine translation model, demonstrating the power and flexibility of the Transformer architecture.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)

## Introduction

The Transformer model has revolutionized the field of natural language processing, providing state-of-the-art performance on various tasks, including machine translation. This project aims to provide a clear, well-documented implementation of the Transformer architecture from scratch, allowing for a deeper understanding of its components and functionality.

## Features

- Full implementation of the Transformer model in PyTorch
- Custom dataset class for bilingual text data
- Attention mechanism implementation
- Positional encoding
- Multi-head attention
- Feed-forward networks
- Layer normalization
- Training and inference scripts
